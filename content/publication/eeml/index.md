---
abstract: To accelerate learning process with few samples, meta-learning resorts
  to prior knowledge from previous tasks. However, the inconsistent task
  distribution and heterogeneity is hard to be handled through a global sharing
  model initialization. In this paper, based on gradient-based meta-learning, we
  propose an ensemble embedded meta-learning algorithm (EEML) that explicitly
  utilizes multi-model-ensemble to organize prior knowledge into diverse
  specific experts. We rely on a task embedding cluster mechanism to deliver
  diverse tasks to matching experts in training process and instruct how experts
  collaborate in test phase. As a result, the multi experts can focus on their
  own area of expertise and cooperate in upcoming task to solve the task
  heterogeneity. The experimental results show that the proposed method
  outperforms recent state-of-the-arts easily in few-shot learning problem,
  which validates the importance of differentiation and cooperation.

url_pdf: https://arxiv.org/pdf/2206.09195
publication_types:
  - "1"
authors:
  - Geng Li
  - Boyuan Ren
  - Hongzhi Wang
author_notes: []
publication: "WISE 2022 accepted. preprint arxiv: https://arxiv.org/abs/2206.09195"
summary: This work is mainly about few-shot training paradigm of integrating
  ensemble learning with gradient-based meta-learning to utilize advantages of
  divergence and cooperation between experts to attain efficacy improvement.
publication_short: Accepted by WISE 2022
title: "EEML: Ensemble Embedded Meta-learning"
doi: " https://doi.org/10.48550/arXiv.2206.09195"
featured: true
tags: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
  filename: image.jpg
date: 2022-07-14T14:57:22.859Z
publishDate: 2022-09-03T00:00:00.000Z
---

